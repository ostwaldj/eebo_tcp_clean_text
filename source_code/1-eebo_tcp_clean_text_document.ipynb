{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning - TCP - v1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to show you how you can use Python to semi-automatically clean Early English Books Online - Text Creation Partnership `txt` files. In a nutshell, the code walks you through common revisions required of many historical texts you'll find in the wild: joining hyphenated words together, replacing common OCR and typing errors, modernizing vocabulary, and so on. At the end of the notebook, you save the resulting output as a new `txt` file, with a few other files logging the changes you've made. Throughout the notebook, the code also saves questionable \"words\" that you might consider looking at more closely to separate files in a `to_check` folder; you can then rerun the notebook and those corrections will be incorporated as well. You can alter the code to meet your particular genre's conventions. Note that if you are dealing with a late 17th-18th century text that you have OCRed yourself, there will likely be other corrections to make; this code will hopefully provide a template for additional revisions. \n",
    "\n",
    "This notebook is intended to provide a template for people in the humanities who know a little bit of Python, but who are unable to find tutorials that focus on the very first, and very necessary, step in digital history - getting your *own* text documents into a form that can then be analyzed. Most online tutorials, targeted at business and (social/data) science audiences, start at the `preprocessing` stage of tokenizing and lemmatizing, but this assumes cleaner input than what is available for most historians. The code below provides one way to massage your text into a clean(er) format that can then be analyzed with all the tools covered in other tutorials.\n",
    "\n",
    "Once you have a \"clean\" version of the text file after this notebook, another notebook should make a number of (optional) transformations to the text. Things like converting number-words to digits, combine together compound words as single terms (e.g. `General_Churchill` instead of `General` and `Churchill` as separate tokens), and converting the period's frequent, and often random, Capitalized Nouns (and Other Parts of Speech) into lowercase nouns, to make named entity recognition and part-of-speech tagging easier. But that's for another notebook.\n",
    "\n",
    "After that second stage, you can then do the normal preprocessing steps discussed in most NLP tutorials: lemmatizing, finding frequent ngrams, POS tagging, and so forth. This first notebook is focused on creating a `txt` file of the *original* document (a big string, in other words), but with corrections and a few other modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For intermediate beginners in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are new to Python, you should look elsewhere for the basics of Python syntax, and of computer programming in general. There are a million (free) resources online, on blogs, and on YouTube. But if you already know those basics, this code provides a variety of techniques that you can use to clean up your own text.\n",
    "\n",
    "In other words, there are several prerequisites in order to use this code: installing Python 3+ and Jupyter Notebooks, for a start (I'd recommend Anaconda) - Google Colaboratory (https://colab.research.google.com/notebooks/welcome.ipynb) is a new intriguing option that, theoretically, allows you to simply import this notebook into it and off you go! If you simply want to substitute your own file for my sample text, you can just swap out filenames. But if you want to modify or extend the code, some background knowledge is required, namely a familiarity with Python objects (strings, lists and dictionaries particularly) and some of their most common methods. An understanding of regular expressions (aka regex) is also key for humanists who want to manipulate text.\n",
    "\n",
    "But this notebook does try to describe and explain the programming choices that I have made: why I chose to use a string `replace` method instead of a regex substitution, or why I did this revision before another one... I also try to show alternative options, in case your use case differs from mine. All these little decisions can make a big difference in the end results, and they aren't necessarily obvious to the beginner. They are also what is most lacking in most online tutorials, which only show you a single way to accomplish your (actually, *their*) task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That said, there are a few important general lessons for how this code deals with text:\n",
    " 1. Read in a substitution lexicon (as a dict) to subsitute one string for another.\n",
    " 2. Use Python's str `replace` method to make substitutions.\n",
    " 3. Use the regex `re.sub` method.\n",
    " \n",
    " Some might be easier than others for any given use case.\n",
    "\n",
    "You should also be aware that there are many different characters with similar appearances, e.g. é vs. è vs. e vs. ê vs. ë... If you're doing crosscultural research that refers to proper nouns from other cultures, you'll need to deal with this at several points. Character encoding is a pain, but life would be so dull if we only had the 128 characters of the traditional English character set to play with. Which means, by the way, you should be using a text editor, and not MS Word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is written as a prototype, to work on a single file at a time, and it is conservative in that it allows you to look at each suggested change before you commit to it. \n",
    "\n",
    "The first thing you should do is set the file paths to your system, specify the file you want to run the code on, and identify whether this is the first time running on a specific file, or whether you're rerunning the code (see below for explanations).\n",
    "\n",
    "Then, if you're the trustworthy type or just want to see what the code does, you can just run all the cells in one fell swoop (in Jupyter notebook: `Kernel-Restart & Run All`) and then check the revised text and the change log for a list of the changes.\n",
    "\n",
    "Assuming you want to explore the possible changes first, each type of edit has several steps:\n",
    "1. Figure out the types of errors you need to fix. The code includes a number of standard types of errors you're likely to find in 17C-18C English texts. You can copy or edit the code to add your own corrections.\n",
    "2. For each error type, there are three basic types of corrections to choose from, each in a separate function. You either correct the error by specifying the string to replace, or you specify the error with a regular expression, or you read in a substitution list that will substitute column A (the error) for column B (the correction).\n",
    "3. The first bit of code in each section will indicate how many of that type of error exist in the document. In a few cases, the errors aren't consistent enough to fix programmatically. These types of errors are saved to separate files that you can look through, make the proper corrections, and then read back into the (end of the) notebook to fix those as well.\n",
    "4. If there are no errors of type X, the code will create an empty dict for audit purposes, letting you know that it looked for error type X but there weren't any.\n",
    "5. If error type X *does* exist in your text, it will show you the errors and run the cell that makes the corrections. Most of the edits are made either with substitution lists (reading a two-column `csv` file in as a dictionary), or with a simple string `replace` method, or using the regular expression `re.sub` method. The edit code also writes the (unique) changes to a separate dictionary (i.e. change log for an audit trail), and prints the keys and values so you can skim through the changes. If mistakes were made, tweak the code and rerun that cell. If the results aren't what you expected, rerun the entire notebook - the order in which you run cells makes a big difference in your results.\n",
    "6. Once you're happy with the edit results, the code cumulatively saves the edits from that section to the text, as a distinct text object.\n",
    "7. Move on to the next type of error and repeat steps 2-6.\n",
    "8. If you need to roll back the edits of the section you are working on, say you need to tweak your regex, just reassign the `text1` variable at the end of the previous section, like `text = text1`. (But don't forget to comment this reassignment out when done.) Or, just rerun the whole notebook again.\n",
    "9. After all the errors have been checked, it saves the output as a `txt` file, the change log as a `csv` file. Look though the change log for any problems - particularly pay attention to the `correct` dict, especially if substring were accidentally substituted, e.g. `particularl` should *not* be changed to `particulars`, for fear of converting `particularly` to `particularsy`.\n",
    "10. Those errors that require individual examination are written to separate files in the `output` folder. You can open each of those potential errors files, look through them, and add the corrections. Then, you can go back to the notebook, change the `Set flag to read in manual lexica`'s `newdoc` value to `N`, then rerun the code to also read in the edited potential errors file and run the substitution function on them.\n",
    "\n",
    "Depending on the size of your text document and your machine, the code might take a few minutes to run.\n",
    "\n",
    "For newbies, remember that your changes are only applied to the text *in memory*, until you explicitly choose to save them to a file. So feel free to iterate through the code, experimenting to see what works best for you - you don't need to worry about overwriting the original `txt` file unless you overwrite that file in the save stage. But if you ever get lost jumping back and forth between sections, just reset by using the `Kernel-Restart & Run` command. If the notebook becomes unresponsive (i.e. a cell won't stop running), use `Interrupt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplemental materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've included about a dozen substitution lexica in the GitHub repo that the code uses to check for errors. Many of these lexica are based off of Ted Underwood's `DataMunging` repository on GitHub:  https://github.com/tedunderwood/DataMunging/tree/master/rulesets. You should modify your copies or use your own, depending on your own corpus. If you have systematic changes to make, remember that you can use some simple Python code to modify these lexica - read them in as a list or a dict and then make whatever changes with regex before writing them back out to a new `csv`/`txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Visualizing English Print project has already done significant cleaning of the EEBO TCP files. They are downloadable from http://graphics.cs.wisc.edu/WP/vep/vep-tcp-collection/; also on GitHub.  But note that some of the corrections below would still need to be done with VEP texts, and you might well be using this code to clean non-VEP texts, so this notebook is based on the original TCP files.\n",
    "\n",
    "For similar tutorials focused on slightly more structured documents, see:\n",
    "1. https://programminghistorian.org/en/lessons/extracting-keywords#build-your-gazetteer\n",
    "2. https://programminghistorian.org/en/lessons/generating-an-ordered-data-set-from-an-OCR-text-file\n",
    "3. https://programminghistorian.org/en/lessons/cleaning-ocrd-text-with-regular-expressions\n",
    "4. https://www.meredithpaker.com/updates/regexcleaning\n",
    "5. https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "\n",
    "The Programming Historian is always a good place to start, though you really should be using Python 3+ and not 2.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've run this code over half-a-dozen different TCP documents, and each one seems to find additional (and sometimes unique) errors to correct. Even transcribed documents will have errors in them, and some of these errors will not be predictable in advance. You will find, for example, different types of character errors in Rohan's *Compleat Captain*, compared to D'Auvergne's *The history of the campagne in the Spanish Netherlands, Anno Dom. 1694 with the journal of the siege of Huy*.\n",
    "\n",
    "So I've chosen to simply add specific corrections for each new documents I run the notebook over, and let the code grow with each new document, rather than try to pare out irrelevant corrections for each document. The `changesdict` audit trail will simply indicate each error the code checked for, and whether there were any changes to it or not. And, frankly, you'll never really know all the types of errors you'll encounter until you look for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use this code with your own documents, pay particular attention to:\n",
    "1. The regular expressions. They can be... challenging, and there are usually many corner and edge cases that are hard to imagine until you discover your text has one. Poorly-formed regular expressions will \"fail silently\", i.e. they'll give you what you asked for, not necessarily what you want, and they'll never kick back an error as an alert. This is why you need to peruse the change log when you're done.\n",
    "2. The lexica of corrections/substitutions. Pay particular attention to the possibility of replacing substrings within an otherwise correct word - you want to balance the desire to find every character error with the desire to find enough context around each error so you can figure out what the erroneous character relly should be. E.g. if you want to replace `Thro` with `Through`, you'll also get `Throughone` instead of `Throne` if you're not careful. So consider padding your strings with spaces, though recognize that, depending on your regex, you might miss some errors that are are the beginning of a line or which are followed immediately by punctuation. For example, `\\S* \\S*\\*\\S* \\S*` will find `hello t*here friend`, but won't find `hello t*here.`\n",
    "\n",
    "In other words, be sure to look through the `changesdict` audit trail whenever you make changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I am still a relative new-comer to Python, and it's my first programming language, this is not expert, or Pythonic, code. But it gets the job done, even if it often adopts a brute-force, repetitive strategy. But that's how you learn.\n",
    "\n",
    "\n",
    "Future improvements would be to further refactor this code to create additional functions for each of the different types of edits (str replace, regex, lexicon as dict), and make it run (automatically) against an entire directory of files. I'm sure the regex could be improved. There are probably also more elegant ways to achieve the goal of cleaning the text - this code allows you to oversee every step of the editing process. And it's conceivable a lot of this could be simplified by tokenizing the text first, but I think these historical transcriptions have too many corner cases for a simple tokenizer. Plus, I really do want to see a cleaned version of the original text, before I start atomizing it for natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to modify this notebook for your own use, there are likely several things you'll need to do:\n",
    "1. Delete undesired edits from the code\n",
    "2. Add your own types of errors to the code\n",
    "3. Curate your own lexica, with substitutions that are common in your corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make some of this notebook's edits with MS Word's find-and-replace feature. You can make more of them with regular expressions and a text editor like Notepad++, TextWrangler, or BBEdit. But using a program like Python will allow you make all these changes more quickly, more flexibly, will create an audit trail which you can refer to later on, and can be rerun on any number of files. It will also allow you to then seamlessly manipulate and analyze your data with dozens of other Python tools, using the same basic syntax, even the same notebook. With thousands of freely-available Python libraries, you can do just about anything you'd like with your text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On to the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step one: load into memory all of the Python libraries you'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T02:10:20.752595Z",
     "start_time": "2018-12-08T02:10:19.768786Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.text import Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which version of Python you are using. This code was made in 3+, and won't be compatible, for example, with Python 2.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which version of NLTK is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change following settings before running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your should set the following for each document, before running the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set basepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are used as short references to the directories/paths you read to/write from. Assigning these variables here means I can refer to them later with the short variable name, rather than the long path. Also, since I use both Mac and PC and since the paths are different on my machines, I have both set up so I can switch back and forth. You should  change them to your own computer paths.\n",
    "\n",
    "You can either type out the paths manually, or do a web search to figure out a way to copy and paste:\n",
    "1. On Windows, you can find the fullpath in the properties.\n",
    "2. You can create an Automator script on the Mac to do the same thing, but there are other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macrootpath = r'YOUR_PATH_HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory for this project. For example, I have a separate directory for each document that I run this code over, so all the files dealing with the Duke of Rohan's *Compleat Captain* is located in the `/rohan_compleatcaptain/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docpath = 'rohan_1640'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mac\n",
    "textpath = macrootpath + '/data/raw/'\n",
    "processedpath = macrootpath + '/processed/'\n",
    "lexicapath = macroothpath + '/lexica/'\n",
    "tocheckpath = macrootpath + '/to_check/'\n",
    "outputpath = macrootpath + '/output/'\n",
    "#win\n",
    "# textpath = winrootpath + docpath + 'data\\raw'\n",
    "# processedpath = rootpath + docpath + '\\processed\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the above to make sure it returns a valid path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the filename as it appears in your directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfilename = \"1640 Rohan Compleat Captain.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a name for your document to programmatically name output files at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'rohan1640'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set flag to read in manual lexica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the errors in the document may require manual correction on your part. This occurs when there is no consistent single correction for a given error, e.g. a character might need to be corrected to several other possible characters. In such cases, the only way to tell which character to correct it to is to look at the context around the error and using your pattern-recognizing brain to figure out the correction. \n",
    "\n",
    "The notebook will save such potential errors to separate `csv` files, which you can then copy as `...list1`, and fix the entries as needed. When you want to incorporate these corrections into the `text`, set the `newdoc` flag below to `N` - the code will then use that test to load in the corrected `list1` and subsitute them.\n",
    "\n",
    "When you first run this code on a document, keep `newdoc` set to `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdoc = 'Y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in document to clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code reads in the text document as an object called `text`. This original `text` will get passed from one section of code to the next, each section making (cumulative) changes to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is an encoding issue (e.g. at the beginning of the printout below, you might find some weird characters in the output, or you get a `codec invalid start byte` error), resave your original text file in a text editor as `UTF-8` (**without** BOM, if you're using a Mac)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T02:10:20.776617Z",
     "start_time": "2018-12-08T02:10:20.759019Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(textpath + textfilename, 'r',encoding='UTF-8') as f:\n",
    "    text = f.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many *characters* does the above text object have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T02:10:20.800174Z",
     "start_time": "2018-12-08T02:10:20.778476Z"
    }
   },
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we begin to check the document for problems, correct them, and keep track of the changes we make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you know what to correct? You could start by comparing every word in your text against a dictionary. But this might give you a LOT of \"errors\", many of which wouldn't actually be errors, but just proper nouns or domain-specific vocabulary that are not in your word dictionary. These could include valid terms like, in my case, the parts of fortifications, obscure military terminology, and so on.\n",
    "\n",
    "So it's probably better to start by skimming through the text and looking for obvious errors. Once you've cleaned a bunch of them programmatically, you can then catch the less common errors by comparing the tokens with a regular dictionary and a lexicon of proper nouns. If you're serious about this, you'll want to create your own collection lexica for your specific domain, particularly people's names, places (toponyms), events, groups, concepts, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I've created a whole bunch of likely errors, likely for the types of documents I want to clean. Your mileage may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `edits` variable to track order of changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following cell creates an `edits` variable that will keep track of which edits you perform, in what order. This `edits` string will be added to the end of your final filename in the output stage, e.g. it might name the file `rohan1640_clean_nb1_aqdfƲo&*q$cp_'vo-hcesh3cfp r2nlpnc_nb1.txt`. This suffix allows an easy way to keep track of which version of your cleaned text you are working with at any point in time. This is important for documenting your method and for replicability, since the order of cleaning steps might affect your results. \n",
    "\n",
    "See: Denny, Matthew, and Arthur Spirling. “Text Preprocessing for Unsupervised Learning: Why It Matters, When It Misleads, and What to Do about It.” SSRN Scholarly Paper. Rochester, NY: Social Science Research Network, September 27, 2017. https://papers.ssrn.com/abstract=2849145.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you don't want that, you can delete that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a copy of your original text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the same copy of `text` will be passed on to the next section, so that the changes are cumulative. If you ever want to compare your corrected version with the original, you can simply reload the cell that reads in the original.\n",
    "\n",
    "At the end of each section, i.e. after each main edit, you can preserve your edited text at that point in the code with a cell `text_edittypeX = text`. Thus, you can roll back changes in the next section of code (if needed) by reversing the order: `text = text_edittypeX`. Easier than having to rerun the entire notebook every time you make too many changes, if you're troubleshooting some of your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the text with its revisions up to that point, just type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text) # or just 'text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute-force technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you didn't care about the details, you could use the following code to 'clean' the text: get rid of everything that isn't a letter, number, space, or basic punctuation, and display the results. Some preprocessing online tutorials even suggest this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[^a-zA-Z\\d\\s\\,\\.;]') # everything that isn't in the bracketed regex\n",
    "text2 = re.sub(pattern,' ',text)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we don't want this - too many new problems, and it's now too difficult to figure out what corrections we *should* have made. Instead, we can use a more detailed process to identify specific problems and then correct them intelligently. It will require much more code, but it is largely automated, with a much more acceptable end result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a series of functions, which we'll use to fix most of the errors. (Some of these might not actually be used in the current version of the notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you put functions at beginning of the notebook, make sure to define all the objects referenced by those functions, e.g. `text`, various change dicts, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `lexiconreplaceassign` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main ways to fix errors is to read into memory a pre-curated list of potential errors, check the `text` for any of these strings, and if it finds any of them, correct that error and add both the error and the correction to the `changesdict`, for audit purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I define a function that takes three variables: `old_text`, `new_text`, and `active_text`. When that function is called, it will take the three variable values you provide and replace them in the replacement, and also add them to the specified change dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this creates a nested dict, with a nested subdict for each type of error. This will require you to use a bit of code to parse different levels of the dict. In general, the `changesdict` looks like this, with each type of error (`vv`, `hyphen`...), followed by a nested dict indicating which specific changes were made:\n",
    "\n",
    "    {'vv': {'this VVork, at': 'this work, at',\n",
    "           'the VVork well,': 'the work well,'},\n",
    "    'weirdo': {},\n",
    "    'hyphen': {'a-breast': 'abreast',\n",
    "               'musket-fire': 'musketfire'}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to take a lexicon, replace the errors for the correction listed in the lexicon, add those error-correction pairs to the `changesdict`, and then assign the resulting string as `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexiconreplaceassign(edittype,outerdict,lexicon,active_text):\n",
    "    for old, new in lexicon.items():\n",
    "        if old in active_text: #keep for pre-curated lexica\n",
    "            active_text = active_text.replace(old,new) #put this before dict assignment \n",
    "            outerdict[edittype][old] = new\n",
    "    return active_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `addextra` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General `addextra` function: replace the errors in `text` (listed in a separate dict) with substitutions specified as an argument in the function call, and add each error and correction to the `changesdict`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addextra(edittype,outerdict,old,replacement,active_text):                      \n",
    "    if old in active_text:\n",
    "        outerdict[edittype][old] = {}\n",
    "        outerdict[edittype][old] = replacement\n",
    "    active_text = active_text.replace(old,replacement) \n",
    "    return active_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variation on the above, if you want to group the errors together by the type of correction to be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addextraenum(edittype,outerdict,old,replacement,active_text,num_list):                      \n",
    "    for i,val in enumerate(squares): #NB not using squares1, because we only need its index number\n",
    "        for j in num_list:\n",
    "            if j == i:\n",
    "                outerdict[edittype][val] = {}\n",
    "                outerdict[edittype][val] = val.replace(old,replacement)\n",
    "                active_text = active_text.replace(val,val.replace(old,replacement))\n",
    "    return active_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `textreplacereplace` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple function that takes the error and the substitution, replaces the error in the `text` and adds them to the appropriate nested dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textreplacereplace(edittype,outerdict,old,replacement,active_text):\n",
    "    if old in active_text:\n",
    "        outerdict[edittype][old] = replacement\n",
    "        active_text = active_text.replace(old,replacement)\n",
    "    return active_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `textreplaceassign` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to use the `str.replace` method and assign the dict value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB:\n",
    "1. This is brute force, not regex. We create a list (regex), then in that list do text replace for text and changedict.\n",
    "2. You don't want to create an empty nested dict here, in case you need to run the code several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textreplaceassign(edittype,outerdict,old_text,new_text,active_text):  \n",
    "    if old_text in active_text:\n",
    "        #outerdict[edittype] = {} #create an empty outerdict; assumes each edittype only uses one function\n",
    "        outerdict[edittype]['old_text'] = {} #create empty innerdict for old value\n",
    "        outerdict[edittype]['old_text'] = old_text #assign old value\n",
    "        outerdict[edittype]['new_text'] = {}\n",
    "        outerdict[edittype]['new_text'] = new_text\n",
    "        active_text = active_text.replace(old_text,new_text)\n",
    "    return active_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start making changes to the text and save them in the `changesdict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete front-matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are focusing on the content of a book, you might want to delete the front matter: title page, publisher info, etc. For example, if you want to extract the toponyms (place names) from the text, you might not want the place of publication to be included in that.  Or maybe you don't want the name of the person to whom the book was dedicated in your list of historical people mentioned in the narrative. It's also worth checking at the end as well, especially for OCRed text boilerplate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, if the goal is to end up with a faithfully rendered version of the text that retains all the important features of the original (important defined by your purpose), we need to balance our desire to automate the corrections as much as possible with the need to not overcorrect. Particularly since we don't always know the exact problems that need fixing, nor the correct corrections. That means we're going to make our changes targeting the most-certain errors and the most-certain correction, which usually means starting with the most precisely-targeted errors and then get more broad. At the end, we can do one last look through the possible errors, and change them individually.\n",
    "\n",
    "The order in which you perform these various corrections may make a difference. Especially challenging is that some words with other errors might need one of the errors to be corrected before the other errors will become evident to our code, e.g. `com- i * pound` might need to have the `i *` deleted (created from stray marks in the right margin of  line 1 and in the left margin of line 2) before the code will know to rejoin the `com-` and `pound` into `compound`. We humans can recognize that problem right away, but there might be hundreds of such errors in a single document, and we don't want to have to correct them all by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some of the errors, e.g. invalid charaters, you *could* create regex patterns to find each character, then use a string replace method to replace every instance of the invalid character with its valid replacement. However, we'd like to keep an audit trail of which changes we made, as well as the entire word in which such an invalid character appeared. This would allow us to make sure the change is legit. So instead of doing a string replace method on the `text`, we'll use regex to identify invalid characters and the word that they belong in, then create a substitution dict that our `lexcionreplaceassign` will work through, adding each change to the `changesdict` audit trail. That way, we'll make the substitutions in the `text` object and have a running audit trail of which words were changed to what."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create `changesdict` for auditing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create an audit trail of the corrections made, we need to create a separate dict to store the old and new values. You could create a separate change dict for each type of change, but instead we'll have a nested dict, so that we have a running tally. Here I make it a plain dict, but for each type of correction, I will create a nested dict inside it.\n",
    "\n",
    "NB: The `edits` string created above is distinct from this change dict; `edits` is only to keep track of the order of the changes and append that string to the filename at the end of the program. That way if the edited text file and the changesdict file get separated, you'll always know the order of edit within the name of the text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the empty dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize on space only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the ways to clean the `text` is to tokenize it, to split each element of the string into tokens. We often think of tokens as individual words, but this will depend on which delimiters you use to tokenize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To standardize the punctuation in the text, we'll need to split our text into tokens. But if you use the default NLTK tokenizer, that uses punctuation as well as whitespace as delimiters. Instead, we'll use the regex `split` function and only split on whitespaces. You wouldn't want to use this tokenize method for most NLP uses, but if all you're looking for is punctuation and some characters around it for context, it'll work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordspace = text.split()\n",
    "wordspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial `CorrectionRules` check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems with cleaning text is that there might be two or more layers of errors in a single word - you need to fix one error before you see that there's another error. For example, `def- scent` might need to be changed to `defcent`, with the code recognizing the pattern of a lowercase letter followed by a dash and space followed by a lowercase letter. Once that error is fixed, the long-s error can be caught, but not before. Thus, the order in which such checks are performed makes a difference. To make this a bit easier, the notebook will perform one of the common checks several times throughout the notebook. This will, for example, decrease the number of errors that get written to the `to_check` lists. It will, however, take a bit longer to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll use Ted Underwood's `Correctionrules` lexicon, which includes 192,000 common errors derived from cleaning tens of thousands of EEBO TCP texts. It is, therefore, a good pre-curated set of targeted corrections that we can take advantage of. I've added numerous additions, based off my own corpus, and also deleted a fair number of corrections that can be dangerous, if you don't pad them with spaces, or if your text uses non-English words. So be sure to check the resulting changes in the `changesdict`, particularly regarding false positive substring hits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the `CorrectionRules` csv as a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (lexicapath + 'CorrectionRules_final_curated.csv','r') as subs:\n",
    "    reader = csv.reader(subs)\n",
    "    correct = {rows[0]:rows[1] for rows in reader} # 1st row as key, 2nd row as value\n",
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty subdict to record all the changes made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['correct'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `lexiconreplaceassign` function on the text, using the `correct` dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('correct',changesdict,correct,text)\n",
    "changesdict['correct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the `c` code, for correct, to the `edits` variable that will be added to the file name. Save an intermediate copy of the edited text to `text_c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'c'\n",
    "text_c = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize Apostrophes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some punctuation marks have a suprising number of variations. If we're using OCRed text, it's possible that the OCR converted them into different versions, such that it thinks there might be more than one type of \"apostrophe\"; another option is that the original source used different types, and the transcriber/OCR retained that distinction. \n",
    "\n",
    "But I don't like that. You can't imagine how frustrating it is trying to figure out why your simple code isn't working, only to find that the problem is actually that you are searching for the wrong \"kind\" of apostrophe!\n",
    "\n",
    "If we want to correct any strings that have apostrophe variants, we should first convert them all to a single type, the standard straight apostrophe - `'`. Then, when we do any searches or replacements, we know that we will get every one of them, rather than worry about which kind of apostrophe we're looking at, or running the same correction across each type of apostrophe. If we want, we can then convert known apostrophes, say, possessives, to one of the other flavors of apostrophe, which would allow us to mass update the straight apostrophe without fear of losing possessives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for an apostrophe in a string, however, is a bit more complicated due to Python's syntax. Since we often use the apostrophe to indicate the beginning and end of a string, that can be confused with the apostrophe as a real character. So we have two options if we want an apostrophe within a string. The first is to use quotation marks - `\" \"` - to indicate the string boundaries; Python doesn't care if the string boundaries are single or double quotes. Alternately, if we use regex, we need to add an extra character. Some characters have special meaning in regex, which means you need to \"escape\" them to let regex know that they are intended as literal characters. You can \"escape\" a literal character by putting a backslash (`\\`) in front of it. In this case, it would look like `'\\''`. Or, you could just do `\"'\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In later sections, we'll be able to automate the corrections: given an string X, we substitute Y for it. But these apostrophes are tricky. We'll want to keep some of them, e.g. the possessives like `Majesty's`, and otherse like elisions: `D'Auvergne`. Some, however, are archaic syncopated contractions of the past tense of verbs (`express'd`) that we'll want to change. We'll deal with them separate in later sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new list, `apost`, for all words with apostrophe variations, based off `wordspace` token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apost = [] #creates an empty list\n",
    "for i in wordspace: #beginning of for each item loop\n",
    "    if re.findall('\\'',i) or re.findall('’',i) or re.findall('`',i) or re.findall('‘',i): #if loop using regex to findall all occurences of x or y...\n",
    "        apost.append(i) #append each item matching above if criteria to the apost list\n",
    "apost #print apost list items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(apost) #how many items are in the list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll probably notice a few other issues that need to be fixed. All in due time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look for the other types of apostrophes, you can also use a `list comprehension` that will put each token with the specific character into a new list, and then append them all into a single list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apost1 = [s for s in wordspace if '’' in s]\n",
    "apost1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apost2 = [s for s in wordspace if '`' in s]\n",
    "apost2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apost3 = [s for s in wordspace if '‘' in s]\n",
    "apost3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above lists are empty, the output is an empty list: `[]`. This indicates that those types of apostrophes do not occur in the given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change `apost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are variant apostrophes to replace, the following code will replace those other types of apostrophes with the straight apostrophe. If the other apostrophe types don't exist, there will be no underlying changes, but you should still run the code below so you'll know that you did check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `textreplaceassign` function, run earlier. We've created 4 arguments in the first line, and all we need to do is pass the value of each argument, making them variables, and then the code will 1) replace the old text with the new text in the text, 2) create a new dict entry for each change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the new nested `edittype` dict that changes will get inserted into, in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['apost'] = {} #create new inner (nested) dict called apost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply call the function by its name, add the values of the arguments for each change, and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aposttochange = ['’','’','‘']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have three types of apostrophes to replace, we could run the function three times, or we can make a list of the apostrophes to change and then loop over that list with the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in aposttochange:\n",
    "    textreplaceassign('apost',changesdict,i,\"'\",text)\n",
    "changesdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there were no changes, the output above should look like: `{'apost': {}}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'a'\n",
    "edits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint save - in case we are working on the next block of cells and want to restart from this point, rather than wait to rerun the entire notebook over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you come across a problem in the next code section and want to backtrack to the end of this section, you can uncomment the line below and run it, starting fresh in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = text_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another tip: if you're working on your code in a long notebook, and want to run your notebook up to (just before) the point which still needs work, you can type something nonsensical into a code cell. Python will error at that cell and stop, allowing you to jump right to where you need to pick up. Personally, I chose `blah` because I couldn't think of anything else to type in the moment, and it's unique, i.e. I can search a notebook for `blah` and I'm unlikely to find a false hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blah #uncomment the beginning of this line if you want the notebook to stop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize Quotation marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with apostrophes, quotation marks also come in several varieties, e.g. `\"`, `“`, and `”`. You should check to see if your text has a single variety, or multiple. You do this in the same way as you did with apostrophes. Again, this may be more important for OCRed text than for hand-entered text like the TCP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, standardize all quotes (e.g. curly quotes) to regular straight quotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, find any double quote marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = []\n",
    "for i in re.findall(r'\\b\\S* {0,1}\\[“|”] {0,1}\\S*\\b',text): #the divisor separator inside a regex [] group means OR\n",
    "    quote.append(i)\n",
    "quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `quotedict` and change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize double quote marks. We can use the same function as above, and just change the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['quote'] = {}\n",
    "textreplaceassign('quote',changesdict,\"“\",\"'\",text)\n",
    "textreplaceassign('quote',changesdict,\"”\",\"'\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['quote']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `edits` abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'q'\n",
    "edits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_q = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = text_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete `∣` divisor and vertical bar `|`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EEBO TCP texts often use the `∣` character to indicate words split across lines. Delete these if you want `Ho∣nourable` to become `Honourable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB1: There are different types of vertical line characters, e.g. this is a `divisor`, not to be confused with a pipe or `vertical bar`, like this: `|`. To be safe, you can add both to your regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB2: The VEP version of TCP eliminated these divisors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we find all instances of words which include the `∣`, to look at what we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisorlist = []\n",
    "divisorlist = re.findall(r'\\b\\S*[|∣]\\S*\\b',text)\n",
    "divisorlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(divisorlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since regex can be quite tricky, it might be worth comparing your expected results with your actual results. If, for example, you want to find a specific character (like `∣`), but you need to find the surrounding characters for the change dict, you can do the following. Search for that character by itself, note the number of occurrences, and then compare that frequency with the number that your more complicated regex returns. For example, if the regex above (`\\b\\S*∣\\S*\\b`) is intended to find *all* divisors, the number that regex returns had better be the same as the number returned if just searching for the divisor sign itself. So let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divis = re.findall(r'[∣|]',text)\n",
    "if len(divis) != len(divisorlist): #if number is different, warn with \"Try again\"\n",
    "    print('Try again')\n",
    "else:\n",
    "    print('Good job')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to delete all of the divisors, we can use our `lexiconreplaceassign` function by creating a substitution dict to send to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisordict = {}\n",
    "for i in divisorlist:\n",
    "    divisordict[i] = re.sub('[|∣]','',i)\n",
    "divisordict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to makes sure it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `divisordict` and change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are assured of our regex, we can make the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make subdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['divisor'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('divisor',changesdict,divisordict,text)\n",
    "changesdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the dict will only contain *unique* keys (i.e. you overwrite the value when the same key is entered a second time), so we shouldn't necessarily assume its length to be equal to `divisorlist` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to check whether all the divisors were deleted, you can search for it in the `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\b\\S*∣\\S*\\b',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your results - if the character is still present, this will create an error that will exit the code and warn you. Useful if you are running, rather than walking, through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_div = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = text_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace `ſ` long-s character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many 17C-18C works used a `ſ` (\"long-s\") character to substitute for an `s`. The \"rules\" were vague and fluid, as this blog post suggests: http://babelstone.blogspot.com/2006/06/rules-for-long-s.html. And I've found exceptions to even those rules, so all bets are off.\n",
    "\n",
    "To allow us to use natural language processing tools, we need to replace them with a normal `s`. VEP already converted them; you'll also come across them in Google Books. Here, we will convert the `ſ` to an `f`, instead of an `s`, on the off-chance that a few might actually be `f` instead of `s`. Don't worry - in a later section of the notebook, we'll read in a list of common long-s words where the `ſ` character has been recorded as an `f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longfslist = []\n",
    "longfslist = re.findall(r'\\b\\S*ſ\\S*\\b',text)\n",
    "longfslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make dict for lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longfsdict = {}\n",
    "for i in longfslist:\n",
    "    longfsdict[i] = i.replace('ſ','f')\n",
    "longfsdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `longfsdict` and change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['longf'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('longf',changesdict,longfsdict,text)\n",
    "changesdict['longf'] #Now that dict getting long, only displaying the current subdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'f'\n",
    "text_f = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ſ' in text:\n",
    "    blah # this is just something to create an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace `Ʋ` character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same basic code for other single-character edits:\n",
    "1. Create a list of the tokens to be changed.\n",
    "2. Figure out what the correct character should be. In this case, `Ʋ` is a `u`.\n",
    "3. Add the old and new tokens to a change dict.\n",
    "4. Use the `replace` method on the entire text.\n",
    "\n",
    "If you want to know what a specific character is called, you can just paste that character into a Google search. But you already knew it is a labiodental approximant, didn't you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VEP deleted these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: There are both upper and lower case versions, and they might have been modified with the case change above. `Ʋʋ`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use regex to find all occurrences of `Ʋ` or `ʋ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labapplist = []\n",
    "labapplist = re.findall(r'\\b\\S*[Ʋʋ]\\S*\\b',text)\n",
    "labapplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labappdict = {}\n",
    "for i in labapplist:\n",
    "    if 'Ʋ' in i:\n",
    "        labappdict[i] = i.replace('Ʋ','U')\n",
    "    else:\n",
    "        labappdict[i] = i.replace('ʋ','u')\n",
    "labappdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `labappdict` and change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['labapp'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call function. Note that we treat lower and uppercase separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('labapp',changesdict,labappdict,text)\n",
    "changesdict['labapp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Ʋnited' in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'Ʋ'\n",
    "text_u = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace `°` character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible that OCR may have misread the letter `o` as a degree symbol, `°`, or maybe even data entry clerks did it. We should check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degreelist = []\n",
    "degreelist = re.findall(r'[a-zA-Z]+°[a-zA-Z]+',text)\n",
    "degreelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the importance of regex here. Just in case you have legitimate `°` signs in your text, the regex above says only to find those that are surrounded by other alphabetical characters. That means that `34° F` will not be flagged; of course, it also means that it would miss something like `hell° there` or `an °utlaw`. Welcome to the hell of regex edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degreedict = {}\n",
    "for i in degreelist:\n",
    "    degreedict[i] = i.replace('°','o')\n",
    "degreedict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `degreedict` and change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change any degree signs to `o`'s, though we'll want to make sure first that they are warranted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['degree'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('degree',changesdict,degreedict,text)\n",
    "changesdict['degree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'o'\n",
    "text_o = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace other weird characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the divisor, labiodental approximate and degree symbol have made you a bit paranoid, they should have. To see what's really out there lurking in our text, we can check for any 'weird' characters that we might need to catch and domesticate. We'll do this with regex. \n",
    "\n",
    "\"Weird\" and \"not-weird\" is all relative, of course, to which language you're using, and what types of documents are in your corpus. A modern historical biography will probably not have many Greek characters, whereas an early modern history might; and we won't even mention early modern alchemical texts... \n",
    "\n",
    "Technically, this is often an issue of charater encoding, particularly a result of the expansion from the earlier ASCII character set (128 possible characters, those most common in English, the main language for early computer programming) to a much much broader character encoding system called Unicode. Search online for details, but this is what allows us to have non-Latin (i.e. non-English) alphabets, as well as millions of different emojis. \n",
    "You can decide, for your particular use case, which characters you want to include and exclude by crafting your regex accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But instead of trying to imagine every possible character that might appear, we'll exclude what we know we *want* from our search. In my case, I want to exclude all of the standard English-language characters listed inside the brackets. The result will display any other characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weird = re.findall(r'[^a-zA-Z\\d\\s\\/\\[\\\\(\\);:\\',.!?\\\\-\\]]',text)\n",
    "set(weird)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see all those characters in context, use a slightly-expanded regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weirdcontext = re.findall(r'\\b\\S*[^a-zA-Z\\d\\s\\/\\[\\\\(\\);:\\',.!?\\\\-\\]]\\S*\\b',text)\n",
    "set(weirdcontext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the results above, we see a few more odd characters that most likely should be something else. Going through the list, we'll need to:\n",
    "1. Decide whether we're ok with having accented characters, or whether we want to 'flatten' them into the ASCII equivalent, e.g. `é` and `è` and `ë` all become `e`.\n",
    "2. Decide what to do about the dashes. Note that there are *multiple* types of dashes - do we want to retain them, or just flatten them into one standard dash? I say flatten.\n",
    "3. Explore a few other characters further, like this weird one: `'̄`. You can look up the name and description of any character by simply selecting it and pasting it into a Google search. \n",
    "\n",
    "So if we were running this on D'Auvergne's campaign narrative of 1694, we'd need to check characters like the ampersand `&`, the asterisk `*`, a macron thingie `'̄`, the circle `•`, the square `▪`, and those angled brackets `〈`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, if we wanted to do the exact same type of correction for all of the above characters, we could make a single bit of code that would walk through each: \"Replace each weird character with whatever in `text` and save the old and new to `changesdict`.\" But, unfortunately, we don't always know what to do with each weird character, and sometimes we need to do different things for a specific weird character based on what letters are around it. This means we'll need to correct these characters one type at a time.\n",
    "\n",
    "You can create new sections of the code below for any additional characters that appear in your source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace `&` ampersand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a start, we might want to replace the `&` - the most obvious solution would be to repalce it with its full version: `and`. But before we do that, however, we should check to make sure that we don't have a `&` for some other reason. For example, some typefaces have a `ct` ligature, which looks an awful lot like `&t` to a computer. So we should check for that *before* we do a batch update of `&`. As this example illustrates, it helps to know your text before you start blindly making changes to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r'\\b\\S*&\\S*',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for `&t` ligature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ampdict = {}\n",
    "for i in re.findall(r'\\S*&[aeiou]\\S*',text):\n",
    "    ampdict[i] = i.replace('&','ct')\n",
    "ampdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find `&` as `etc.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in re.findall(r'&c',text):\n",
    "    ampdict[i] = i.replace('&c','etc')\n",
    "ampdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the empty subdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['ampersand'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `lexiconreplaceassign` function to make the substitution in `text` and add them to `changesdict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('ampersand',changesdict,ampdict,text)\n",
    "changesdict['ampersand']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you didn't find any `&t` ligatures in your document, we *could* go on to changing all those `&` to `and`. But maybe we should make sure by using a broader regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r'\\S*&\\S*',text) #\\S* means 'zero or more non-whitespace characters'. Narrower would be '[a-zA-Z]&[a-z]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check for any remaining `&`. If we include padded spaces on either side, we'll find any `&` by themselves, which presumably means they should be `and`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r' & ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampdict1 = {} #NB: instead of a new ampdict1, we could add this to the previous ampdict and then call the function after\n",
    "for i in re.findall(r' & ',text): #NB: here we skipped creating the amplist and just put the regex in our for loop, since findall returns a list\n",
    "    ampdict1[i] = i.replace(' & ',' and ')\n",
    "ampdict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('ampersand',changesdict,ampdict1,text)\n",
    "changesdict['ampersand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += '&'\n",
    "text_a = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `*` Asterisk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With poor-quality OCR, some characters or stray marks may be interpreted as `*`. Check to see if those need correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all `*` that are connected to a word. Note that since the `*` character is also a special class in regex, i.e. it's shorthand for \"find the previous character zero or more times\", we need to 'escape' it, by adding a slash `\\` in front of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r'\\S*\\*\\S*',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could expand the regex and see a bit more context around these asterisk words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r'\\S* \\S*\\*\\S* \\S*',text) # Each <space>\\S*<space> adds another word of context: 'any non-whitespace, zero or more times'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the results, we see that sometimes we want to delete the `*`, other times we might want to replace it with a space. We might even want to retain the `*` if it's being used as a footnote marker. To keep it simple, I'll use a conditional `if` statement: either replace the `*` with nothing (`''`) if it's followed by a space, otherwise replace the `*` with a space (`' '`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create replacement dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "astdict = {}\n",
    "for i in re.findall(r'\\S* \\S*\\*\\S* \\S*',text):\n",
    "    if re.search(r'\\S* \\S*\\* \\S*',i): # if * followed by space\n",
    "        astdict[i] = i.replace('*','')\n",
    "    else: # could also make elif with regex, to be more targeted\n",
    "        astdict[i] = i.replace('*',' ')\n",
    "astdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(astdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['asterisk'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('asterisk',changesdict,astdict,text)\n",
    "changesdict['asterisk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've made the above corrections, we can craft a broader regex to search for any other `*` we might have missed - our regex above required having spaces and non-whitespace characters on either side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\S*\\*\\S*',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could change these generically: replace the remaining `*` with a space. Extra spaces aren't a major concern. If desired, we can convert all multiple-space sequences to a single space later in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astdict1 = {}\n",
    "for i in re.findall(r'\\S*\\*\\S*',text):\n",
    "    astdict1[i] = i.replace('*',' ')\n",
    "astdict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('asterisk',changesdict,astdict1,text)\n",
    "changesdict['asterisk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += '*'\n",
    "text_a = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace `$` with `s`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously dollar signs should only be deleted for particular corpora, depending on your geographical and chronological focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think there might be many, you can start with a more precise search, worried that you'll need to make different changes, depending on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T02:10:35.018850Z",
     "start_time": "2018-12-08T02:10:20.396Z"
    }
   },
   "outputs": [],
   "source": [
    "re.findall(r'\\b\\S*\\$\\S*\\b',text) #NB need to escape $ in regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if none appear in your narrow search, or if you want to start with a broader search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\$',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a lexicon dict and add any changes to the `changesdict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollardict = {}\n",
    "for i in re.findall(r'\\b\\S*\\$\\S*\\b',text):\n",
    "    dollardict[i] = i.replace('$','')\n",
    "dollardict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['dollar'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('dollar',changesdict,dollardict,text)\n",
    "changesdict['dollar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += '$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_S = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace `▪` Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other 'gremlins' might get added. Check to see if these characters can simply be deleted (easiest), or if they need to be substituted for another character (harder, unless it's always the same character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'▪',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the document, there might be a lot of squares, so let's get a bit more context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "square = re.findall(r'\\S* \\b\\S*[▪]\\W*\\S*\\b \\S*',text)\n",
    "square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: Remember that you can get more context by extending the regex with more `\\S* ` inside the word boundaries (`\\b`) on each side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some documents, this character is rather problematic, since some need deletion and other need spaces, or dashes, or even other letters. If there aren't that many items in `square` (you can return the `len` if you make the results a list), and if you're a stickler for detail, you can do each one separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where pattern recognition comes in. Maybe you notice that some of the squares should be replaced with a period. Maybe you realize that the following rule will find those where the square is preceded by a lowercase letter and then followed by a space, and then an uppercase letter (note that that uppercase letter is important, otherwise the `Maestricht▪ and` would be converted to a period, like `Maestricht. and`). More problematically, maybe, upon further reflection, you realize that something like `Fortification▪ Count` could be something like `Moving toward the Fortification Count Horn briskly...`. In other words, I'm not sure how to automate this type of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option would be to manually add each errors and corrections. You could simply copy & paste each line, and edit the `old` and `replacement` values in each line. But this would need to be changed for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if newdoc == 'N':\n",
    "    # text = addextra('square',changesdict,'12th▪','12th',text)\n",
    "    # text = addextra('square',changesdict,'Maestricht▪','Maestricht',text)\n",
    "    # changesdict['square']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we'll create a separate file with all the square errors, make the appropriate corrections in a text editor, then read them back in as a lexicon to substitute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'll:\n",
    "1. Write `square` list to separate files, one original and one copy (`square1`). The copy will avoid overwriting if we need to modify code\n",
    "2. Ignore any items we don't want to change (quicker than deleting the false positives)\n",
    "3. Correct those you do want to change\n",
    "4. Read this `square1` back in as a dict\n",
    "5. Run the `lexiconreplaceassign` function on this new dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squaredict = {}\n",
    "for i in re.findall(r'\\S* \\b\\S*[▪]\\W*\\S*\\b \\S*',text):\n",
    "    squaredict[i] = i\n",
    "squaredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tocheckpath + filename + '_square.csv', 'w', encoding = 'UTF-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for k,v in squaredict.items():\n",
    "        writer.writerow([k, v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another copy that you'll edit. Note that this will **overwrite** any changes to a previous version of `square1`, we'll add an `if` statement where `newdoc == 'Y'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if newdoc == 'Y':\n",
    "    with open(tocheckpath + filename + '_square1.csv', 'w', encoding = 'UTF-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for k,v in squaredict.items():\n",
    "            writer.writerow([k, v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['square'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've changed the items in text editor, you reset the `Set flag` newdoc to `N` and run the rest of the notebook. The `if` statement will run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['square'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if newdoc == 'N':\n",
    "    with open (tocheckpath + filename + '_square1.csv','r') as subs:\n",
    "        reader = csv.reader(subs)\n",
    "        square1 = {rows[0]:rows[1] for rows in reader}\n",
    "    text = lexiconreplaceassign('square',changesdict,square1,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "changesdict['square']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be one or two others, given the regex we used above. This should get caught by later code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r'\\S*▪\\S*',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if newdoc == 'N':\n",
    "    edits += '▪'\n",
    "    text_s = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete `•` and `·` circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all circles, with some surrounding context - on the assumption that circles `•` might be as complicated as their similarly-shaped square `▪` cousins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(re.findall('[•·]',text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any circles with non-whitespace character around them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circle = re.findall(r'\\S* \\b\\S* {0,1}[•·] {0,1}\\S* \\S*\\b',text)\n",
    "circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As feared, here too, we might need to do different things with the circles, requiring separate edits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `circledict` and change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the same process as with squares: save as `circles`, rename as `circles1`, make corrections, read back in and substitute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circledict = {}\n",
    "for i in re.findall(r'\\S* \\b\\S* {0,1}[•·] {0,1}\\S* \\S*\\b',text):\n",
    "    circledict[i] = i\n",
    "circledict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tocheckpath + filename + '_circle.csv', 'w', encoding = 'UTF-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for k,v in circledict.items():\n",
    "        writer.writerow([k, v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if newdoc == 'Y':\n",
    "    with open(tocheckpath + filename + '_circle1.csv', 'w', encoding = 'UTF-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for k,v in circledict.items():\n",
    "            writer.writerow([k, v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['circle'] ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if newdoc == 'N':\n",
    "    with open (tocheckpath + filename + '_circle1.csv','r') as subs:\n",
    "        reader = csv.reader(subs)\n",
    "        circle1 = {rows[0]:rows[1] for rows in reader}\n",
    "    text = lexiconreplaceassign('circle',changesdict,circle1,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['circle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any missed by the regex should get caught later on. Or, you could improve the regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if newdoc == 'N':\n",
    "    edits += 'c'\n",
    "    text_c = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete angled brackets `〈`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angled brackets could also be a problem. Here we do something very specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['blankpage'] = {}\n",
    "text = addextra('blankpage',changesdict,'〈1 page duplicate〉',' ',text)\n",
    "changesdict['blankpage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_bp = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace `?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the question mark is a valid choice to end a sentence - or is it? Other times, it might be used by a data entry person to indicate an uncertain character. In the EEBO TCP, some of the documents have many question marks that we'd like to get rid of. Let's find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's just get a quick count of all the `?`s that appear in `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\?',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(re.findall(r'\\?',text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find any `?` inside a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quest = []\n",
    "for i in re.findall(r'\\S*\\?[\\w]\\S*',text):\n",
    "    quest.append(i)\n",
    "quest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(quest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these shouldn't be correct, we'll delete them. But you should probably look through the `changesdict`, because it's possible the `?` is actually a substitute for another letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create replacement dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questdict = {}\n",
    "for i in quest:\n",
    "    questdict[i] = i.replace('?','')\n",
    "questdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['quest'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('quest',changesdict,questdict,text)\n",
    "changesdict['quest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(changesdict['quest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten the easy ones out of the way, how many are left? NB: Since dict keys are unique, there may have been more words corrected than in the len above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(re.findall(r'\\?',text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find those that seem legitimate, i.e. are at the end of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is most likely a better regex to find more of the `?`, but this is an ugly start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questcheck = []\n",
    "for i in re.findall(r'[\\s\\S*]\\S*\\?[\\s\\S]*? \\S*',text):\n",
    "    questcheck.append(i)\n",
    "questcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(questcheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's send spin them off to a separate file for a closer look. Some of these might take more investigating, even look at the context in the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questcheckdict = {}\n",
    "for i in questcheck:\n",
    "    questcheckdict[i] = i\n",
    "questcheckdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tocheckpath + filename + '_questcheck.csv', 'w', encoding = 'UTF-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for k,v in questcheckdict.items():\n",
    "        writer.writerow([k, v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if newdoc == 'Y':\n",
    "    with open(tocheckpath + filename + '_questcheck1.csv', 'w', encoding = 'UTF-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for k,v in questcheckdict.items():\n",
    "            writer.writerow([k, v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['questcheck'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if newdoc == 'N':\n",
    "    with open (tocheckpath + filename + '_questcheck1.csv','r') as subs:\n",
    "        reader = csv.reader(subs)\n",
    "        questcheck1 = {rows[0]:rows[1] for rows in reader}\n",
    "    text = lexiconreplaceassign('questcheck',changesdict,questcheck1,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'q'\n",
    "text_q = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert `¶`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible there are paragraph marks within your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r'\\S*¶\\S*',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paradict = {}\n",
    "for i in re.findall(r'\\S*¶\\S*',text):\n",
    "    paradict[i] = i.replace('¶','')\n",
    "paradict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['para'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('para',changesdict,paradict,text)\n",
    "changesdict['para']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'p'\n",
    "text_p = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other weird (non-ASCII) characters to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've made targeted corrections, let's see what other non-ASCII characters we need to deal with. Even if we want to keep some accented characters, there are probably others that we should change, because they are most likely OCR/entry errors. For example, my texts are primarily in English and French circa 1700, which means that there are many non-English characters that contemporaries would *not* have used. We can search for them with regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as an example, to find any foreign `c` characters: `ćĉċč`, excluding the French `ç`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\b\\S*[ćĉċč]\\S*\\b',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we won't necessarily know *which* weird characters will be in any given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple way to find all the non-ASCII characters is to use the `findall` method for all non-ASCII characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nonascii = set(re.findall(r'[^\\x00-\\x7F]',text))\n",
    "nonascii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you could have actually done this earlier in this notebook, and you would have found the divisor, the degree sign, and so on. But you want to keep separate those character errors that can be programmatically corrected (e.g. `Ʋ` always is a `U` - have the code automatically correct them) with character errors that require individual attention. Generally, make specific, targeted, corrections before general braod-brush changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that the other character errors are unknown, at this stage. Let's find them and read them into a dict with the error in both key and value. We'll then write that dict out to a separate file, and we can just change the single character error, instead of retyping the entire key. Open file, make corrections manually, read back in as lexicon and run to correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(re.findall(r'\\S*[^\\sa-zA-Z0-9,\\.\\?;:\\'\\\"\\(\\)\\-!\\áàäéèëîïíöü]\\S*',text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nonasciidict = {}\n",
    "for i in re.findall(r'\\S* \\S*[^\\x00-\\x7F]\\S* \\S*',text):\n",
    "    nonasciidict[i] = i\n",
    "nonasciidict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nonasciidict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the document, some of these characters may be replaced with a single character. Others, however, may require much more effort, since a single weird character might need to be replaced with many different characters, depending on the instance. The `ï` might be `o`, `i`, `s`, `e`, `u`..., depending on the context!\n",
    "\n",
    "If you do see a pattern,  e.g. many of the `ï` are `i`, you can do a regex replace, either here or in your text editor, and then manually correct those few that were corrected to `i`, but should actually be something else. You could also check to see if some of the words repeat themselves, e.g. maybe `Generalï` appears multiple times. Generally, try to minimize the amount of time you spend doing hand-correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time being, we can write this list to a file, that we can look at and decide what to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tocheckpath + filename + '_unicode_to_check_list.csv', 'w', encoding = 'UTF-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for k,v in nonasciidict.items():\n",
    "            writer.writerow([k, v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if newdoc == 'Y':\n",
    "    with open(tocheckpath + filename + '_unicode_to_check_list1.csv', 'w', encoding = 'UTF-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for k,v in nonasciidict.items():\n",
    "                writer.writerow([k, v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have manually corrected the `check_list.csv`, we can change the `newdoc` value to `Y` (at the beginning of the notebook), and then rerun the entire notebook. This will run the code below, which will run the following two cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['unicode'] ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if newdoc == 'N':\n",
    "    with open (tocheckpath + filename + '_unicode_to_check_list1.csv','r') as subs:\n",
    "        reader = csv.reader(subs)\n",
    "        unicode = {rows[0]:rows[1] for rows in reader} # 1st row as key, 2nd row as value\n",
    "    text = lexiconreplaceassign('unicode',changesdict,unicode,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['unicode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if newdoc == 'N':\n",
    "    edits += 'u'\n",
    "    text_u = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add space between punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes punctuation might need to be padded;like this. Do apostrophes separately, because they are complicated by things like elision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall('\\S*[a-z][,;][a-z]\\S*',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "puncdict = {}\n",
    "puncpatt1 = re.compile(r'(\\S*[a-z][,;])([a-z]\\S*)')\n",
    "for i in re.findall(r'\\S*[a-z][,;][a-z]\\S*',text):\n",
    "    puncdict[i] = re.sub(puncpatt1,r'\\1 \\2',i)\n",
    "puncdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['puncspace'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('puncspace',changesdict,puncdict,text)\n",
    "changesdict['puncspace']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += '_'\n",
    "text__ = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicate apostrophes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce duplicate apostrophes to a single apostrophe, to be cleaned later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplapost = {}\n",
    "for i in re.findall(r'\\b\\S*\\'{2,} [a-zA-Z]\\S*\\b',text):\n",
    "    duplapost[i] = i.replace(\"''\",\"'\")\n",
    "duplapost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['duplapost'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('duplapost',changesdict,duplapost,text)\n",
    "changesdict['duplapost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += \"'\"\n",
    "text_a = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert double-letter to single letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dangerous, given some legit double-letter words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubleletter = re.findall(r'\\b([a-zA-Z])\\1{2}',text)\n",
    "doubleletter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be worked on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert `vv` to `w`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few double-letters that we want converted to a known single character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vvdict = {}\n",
    "for i in re.findall(r'\\S* \\S*[vV]{2}\\S* \\S*',text):\n",
    "    if 'vv' in i:\n",
    "        vvdict[i] = re.sub('[v]{2}','w',i)\n",
    "    else:\n",
    "        vvdict[i] = re.sub('[V]{2}','W',i)\n",
    "vvdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['vv'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('vv',changesdict,vvdict,text)\n",
    "changesdict['vv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'v'\n",
    "text_vv = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ó`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This letter might be extraneous (delete), but might also actually be an `o` - how distinguish with code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another correction to think about before automating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weirdo = re.findall(r'\\b\\S*ó\\S*',text)\n",
    "weirdo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weirdodict = {}\n",
    "for i in re.findall(r'\\b\\S*ó\\S*',text):\n",
    "    weirdodict[i] = i.replace('ó','o')\n",
    "weirdodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['weirdo'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('weirdo',changesdict,weirdodict,text)\n",
    "changesdict['weirdo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'o'\n",
    "text_o = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize Dashes and Hyphens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correcting dashes is a more challenging task. As with the previous punctuation, there are multiple variants of the dash/hyphen. In fact, there are *four* different dash characters (hyphen `-`, minus-sign `−`, em dash `—`, en dash `–`) that are practically indistinguishable to the untrained eye. But OCR/entry might interpret the same character differently from instance to instance, and sometimes printers would use them in different scenarios as well. So depending on your purpose and genre of text, you might want to keep some of the dashes, but delete others. Or, in this case, you just want to convert them all to the same type of dash.\n",
    "\n",
    "Further complicating matters, there will be a lot of dashes in published texts: compound toponyms like `Saint-Omer` or people like `Jean-Claude`. French and foreign phrases might have correct dashes to keep, `n'est-ce pas?`. \n",
    "Summarizing acceptable uses of dashes:\n",
    "1. Foreign phrases (`est-ce que`, `n'est-ce pas`)\n",
    "2. Names (`Mérode-Westerloo`, `Newcastle-upon-Tyne`)\n",
    "3. Year ranges (`1700-1704`)\n",
    "4. Titles (`aide-de-camp`...)\n",
    "5. British spellings (e.g. `co-opt`)\n",
    "\n",
    "Depending on the quality of the OCR/transcription, you might also find random dashes (e.g. `the-only`) that require correction. But you don't want to make a mass update of every dash between two letters, for fear of deleting the acceptable dashes mentioned above.\n",
    "\n",
    "For early modernists using EEBO/ECCO, we discover that since the Text Creation Partnership was interested in the physical layout of the printed page, as well as the text on it, they faithfully recorded the hyphenated splits of words across lines. You'll find the same on OCRed text. You can use regex to catch simple version like `compound` thus: `com-\\npound`, or, more generally, `[a-z]-\\n[a-z]` will find every dash and line break surrounded by a lowercase letter on either side. VEP has eliminated those.\n",
    "\n",
    "Unfortunately, those line breaks may have further complications, particularly if you have OCRed text. As mentioned earlier, tight book bindings can lead to dark shadows in the margin/gutter on either side of the page, which might be interpreted as characters, which could require your regex to find a scenario like this: ` com- i`\n",
    "\n",
    "`* pound`. \n",
    "\n",
    "A word might be split across more than one line break, if, for example, the word breaks across two pages. In that case, you'd need to clear out the header and footer text first, as well as any gutter cruft, before rejoining the split word.\n",
    "\n",
    "In other words, there are a few issues to address. So you might want to treat them differently, and in steps. We'll do the following:\n",
    "1. Standardize all the dashes. I'm not interested in dash variations.\n",
    "2. Then, we can use a substitution lexicon to replace dashes. In some instances, we'll want to simply delete the dash; others might require converting the dash into a space, and some might convert the dash into an underscore if you want to group words together, distinguishing `Saint_Omer` from `Saint Denis`. This underscoring will be done in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize dashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize dashes in full text string, so that if you search for a 'dash', you will find everything that looks like a dash, regardless of how it is used. But if your document uses dashes intelligently, skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, find all normal (minus sign, or `-`) hyphens, to see the variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dash = []\n",
    "for i in re.findall(r'\\b\\S* {0,2}- {0,2}\\S*\\b',text):\n",
    "    dash.append(i)\n",
    "set(dash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the above hyphens might be valid uses, e.g. ranks and entities, while others could be archaic hyphenations that we'll want to add to our substitution list, e.g. `Public-Houses` to `public houses` (or possibly `public_houses`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, make a list for all non-hyphen words that we should standardize. (In case we worry about dashes interacting with tokenization, we can just use regex to search over the entire `text` string.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'[−—–]',text) #the 4 types of dashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonhyphen = []\n",
    "for i in re.findall(r'\\b\\S*\\W*[−—–]\\W*\\S*\\b',text): # escape the hyphen in regex\n",
    "    nonhyphen.append(i)\n",
    "nonhyphen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like this type of dash is (mostly) used between a line break (`\\n`) and a tab (`\\t`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nonhyphen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize all dashes to hyphen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `dashdict` and standardize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the variety of options, this is probably just as easy to code in an `if...elif` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'(\\S* {0,1})[–−−]( {0,1}\\S*)') # regex pattern to use later in code; () groups into 2 groups\n",
    "nonhyphendict = {}\n",
    "for i in nonhyphen:\n",
    "    if '−' in i:\n",
    "        nonhyphendict[i] = i.replace('−','-')\n",
    "    elif '—' in i:\n",
    "        nonhyphendict[i] = i.replace('—','-')\n",
    "    elif '–' in i:\n",
    "        nonhyphendict[i] = i.replace('–','-')\n",
    "text = re.sub(pattern,r'\\1\\2',text) # after made change dict, replace pattern with groups 1 & 2 of pattern (i.e. without space)\n",
    "edits += \"-\"\n",
    "nonhyphendict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nonhyphendict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_d = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've standardized the dashes, we can start replacing them more systematically. We'll use a large lexicon derived from Ted Underwood's `DataMunging` work: https://github.com/tedunderwood/DataMunging/tree/master/rulesets. It's one of many `csv` files that are formatted thus: `incorrect string,correct string`. These values are read into a dict, and then used to substitute the incorrect for the correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these types of lexica, there are several complications that require some thought:\n",
    "1. Whether you pad the entries with spaces (leading/trailing) or not. This is particularly important regarding substrings within other words. For example,`_ ham_` (underscore indicating a space) would not match `I_ate_ham.`, with ham at the end of a sentence. But finding and replacing every instance of `him` with `ham` (without the padded spaces) would also substitute the substring `ham` in `sham` and `hamlet`, resulting in `shim` and `himlet`. Unpadded substitutions might change many more strings than you intend; on the other hand, padded substitutions may not match every instance you want to change, due to surrounding punctuation.\n",
    "2. The order in which you run your edits, i.e. the order in which entries are sorted in your lexicon. If you don't use padded spaces, consider running more precise (i.e. longer strings) first, e.g. `_hamlet_`, before shorter strings with `ham` in them.\n",
    "3. Possessives, plurals, etc. If you include trailing spaces, you will not catch plurals and the like. Normally you would deal with this problem by tokenizing your text and then lemmatizing it (e.g. converting all verb forms into the infinitive...). But if your text is really dirty and if you want to create a clean `txt` version, it might have unpredictable results. Another option is to duplicate your lexicon entries in the substitution list and then add the endings. You can even do this using some Python code.\n",
    "4. Capitalization - do you need a separate capitalized version of each old/new pair (`hello`, `Hello`)? You could make everything lower case (`lower` method), but then you'd lose any capitalized words that you might want to identify later, e.g. proper nouns with named entity recognition. I went with having both lower and title case in my lexica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways you can deal with these: wrap the substring inside word boundaries (`\\b`), make the larger string substitutions before the smaller ones, etc. Trial and error, and some regex research, is your best guide. Be sure to check out the results before you go on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Substitute `hyphen2concat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This substitutes any hyphenated words into single words, based off Underwood's original lexicon. Use the separate `hyphen2underscore` dict to retain hyphens but convert them to an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyphenreplace = {}\n",
    "with open (lexicapath + 'hyphen2concat.csv','r') as subs:\n",
    "    reader = csv.reader(subs)\n",
    "    hyphenreplace = {rows[0]:rows[1] for rows in reader}\n",
    "hyphenreplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hyphenreplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all the items from the above dict that are in `text`, i.e. items to be changed. If there are items in this dict that you don't want to (ever) be changed, you can delete them from the lexicon file, and reload the previous code. But then you'll need to think about whether you want to keep multiple versions of each lexicon or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyphendict = {}\n",
    "for k,v in hyphenreplace.items():\n",
    "    if k in text:\n",
    "        hyphendict[k] = v\n",
    "hyphendict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hyphendict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `lexiconreplaceassign` function for hyphens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['hyphen'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('hyphen',changesdict,hyphendict,text)\n",
    "changesdict['hyphen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check through the results and make sure there aren't any problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.count('Breast-work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_d = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check punctuation more broadly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're paranoid, we can look for words with specified punctuation (in square brackets in the regex) inside them.\n",
    "\n",
    "See apostrophes below, along with elisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T02:10:34.990198Z",
     "start_time": "2018-12-08T02:10:20.290Z"
    }
   },
   "outputs": [],
   "source": [
    "punc = re.findall(r'\\b\\S*[a-zA-Z][•!\"#$%&()*+,./:;<=>?@[\\]^_`{|}~]\\S*\\b',text)\n",
    "for i in punc:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, create an empty `changesdict['punc'] = {}` and manually add any unpredictable errors using `addextra`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title case UPPERCASE words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the edits are best done by tokenizing, i.e. breaking down the uninterrupted `text` string into tokens, but only to find the errors - you'll create a changedict based off the tokens, and then make the changes to `text`. Often these can be thought of as words, but depending on the tokenizer, they can also include punctuation, and words can even be split, e.g. the letters after apostrophes like `'s`.\n",
    "\n",
    "There are many different types of tokenizers, and they are useful for particular tasks. NLTK's default word tokenizer will split text strings at every whitespace *or* punctuation. That means possessives (e.g. `Dean` and `s` from `Dean's`) and contractions (`can` and `t` from `can't`) will be separated. This may be useful depending on the type of analysis, but for cleaning text, we'd rather it not tokenize based off apostrophes. \n",
    "\n",
    "This means that we can clean our text either based off of the text string, or off of the tokens, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasinally there will be words in uppercase, i.e. all caps. Decide if you want to convet them to Titlecase or not. You can check for words that are in ALL CAPS (greater than length of 2 so avoid `I`, `M.`, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few issues worth thinking about, scenarios in which you might want to keep multiple uppercase letters:\n",
    "1. M. (Monsieur de ) or MM. for Messieurs.\n",
    "2. A. (first name abbreviation) or A.M. \n",
    "3. Roman numerals (XVIII...).\n",
    "4. Maybe used for EMPHASIS, which might be something you're interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify case, we'll retokenize `text` (the current version) and then check for any tokens that were upper case and longer than two characters. Skim through the resulting list to see if there is anything out of place. \n",
    "\n",
    "If you are concerned about the tokenizer acting weird, you shouldn't rely on `words`, but instead should do a search across `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = nltk.word_tokenize(text)\n",
    "tokens1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use list comprehension to only keep non-puncutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = [i for i in tokens1 if i not in (',','-',';',':','.','’','&','#','$','!','%','\\'','*','•','(',')')]\n",
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allcap = []\n",
    "for i in words1:\n",
    "    if i.isupper() and len(i) >2:\n",
    "        allcap.append(i)\n",
    "allcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allcap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there may be additional errors in the above list. They might be fixed with later code, but you can also make a note of them just in case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roman numerals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be Roman numerals (XIV...), so we should figure out a way to exclude those from our substitution, i.e. we want to keep them UPPERCASE. We could identify each Roman numeral and exclude it from the `allcap` list, or we could figure out a pattern to use to identify only Roman numerals. \n",
    "\n",
    "Since there are only a few, we'll just `remove` them from the `allcap` list, and check the resulting `len` to make sure no substrings were accidentally matched. But the `remove` method only removes the first item from the list, and we might have more than one instance of the same item, e.g. `XIV` appears twice. We could make a `for` loop to remove each item: \n",
    "\n",
    "    for i in allcap:\n",
    "    if i == 'XIV':\n",
    "        allcap.remove('XIV')...\n",
    "        \n",
    "But, I just happen to have a list of Roman numerals up to 99 (the things you find on the Internet), so we can import that in as a (padded) lexicon and call our usual `lexiconreplaceassign` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open (lexicapath + 'roman_numerals.txt','r') as f:\n",
    "    roman = f.read().split('\\n')\n",
    "print(roman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for romnum in roman:\n",
    "    for cap in allcap:\n",
    "        if ' ' + romnum + ' ' == ' ' + cap + ' ':\n",
    "            allcap.remove(cap)\n",
    "allcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allcap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace uppercase words with Titlecase words, using `allcap` list. We can't use the `lexiconreplaceassign` function, since we can't change every instance to the same exact replacement, so we'll need to make a bit of code to take the characters for each item and just convert that word to Titlecase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['allcap'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in allcap:\n",
    "    changesdict['allcap'][i] = i.title() # set dict key to i and value to titlecase i\n",
    "    text = text.replace(i,i.title()) # replace in text\n",
    "edits += 'c' # take existing value of edits & add c (cap) to it\n",
    "changesdict['allcap']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can double-check for any UPPERCASE that were missed. They should only be strings that didn't meet the `allcap` criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'[A-Z]{2,}',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can change those as well, but, for me, they're fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_u = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elisions and stray apostrophes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another use for apostrophes is to combine together words so they flow better, in French, for example. We want to identify and preserve those. We can find them searching `text` with a regular expression.\n",
    "\n",
    "But, sometimes, we might need to delete apostophe rather than combine it. So let's start narrow and limit our regex to known elision characters: `l`, `d`, `m`... followed by capitalized words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for possible elisions requiring compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "elisiontocompress = {}\n",
    "for i in re.findall(r'\\b[ldmLDOM]\\' [AEIOU]\\S*\\b',text): # add other letter combos as needed\n",
    "    elisiontocompress[i] = re.sub(\"\\' \",\"\\'\",i)\n",
    "elisiontocompress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['elision'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('elision',changesdict,elisiontocompress,text)\n",
    "changesdict['elision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for possible apostrophes requiring deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for apostrophes with spaces on either side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apostspace = {}\n",
    "for i in re.findall(r'\\b\\S* \\' [a-zA-Z]\\S*\\b',text):\n",
    "    apostspace[i] = re.sub(\" \\' \",\" \",i)\n",
    "apostspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['apostspace'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('apostspace',changesdict,apostspace,text)\n",
    "changesdict['apostspace']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for apostrophes padded left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostspaceL = {}\n",
    "for i in re.findall(r'\\b\\S* \\'[a-zA-Z]\\S*\\b',text):\n",
    "    apostspaceL[i] = re.sub(\" \\'\",\" \",i)\n",
    "apostspaceL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['apostspaceL'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('apostspaceL',changesdict,apostspaceL,text)\n",
    "changesdict['apostspaceL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for apostspace right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\b\\S*\\' [a-zA-Z]\\S*\\b',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here too, the changes could go multiple directions. Let's save yet another list, edit it, reread it back in, and then substitute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostspace = {}\n",
    "for i in re.findall(r'\\b\\S*\\' [a-zA-Z]\\S*\\b',text):\n",
    "    apostspace[i] = i\n",
    "apostspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tocheckpath + filename + '_apostspace.csv', 'w', encoding = 'UTF-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for k,v in apostspace.items():\n",
    "        writer.writerow([k, v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if newdoc == 'Y':\n",
    "    with open(tocheckpath + filename + '_apostspace1.csv', 'w', encoding = 'UTF-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for k,v in apostspace.items():\n",
    "            writer.writerow([k, v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['apostspaceR'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if newdoc == 'N':\n",
    "    with open (tocheckpath + filename + '_apostspace1.csv','r') as subs:\n",
    "        reader = csv.reader(subs)\n",
    "        apostspaceR = {rows[0]:rows[1] for rows in reader} # 1st row as key, 2nd row as value\n",
    "    text = lexiconreplaceassign('apostspaceR',changesdict,apostspaceR,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['apostspaceR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'e'\n",
    "text_e = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check remaining hyphens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display hyphens not fixed already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dashleft = []\n",
    "for i in re.findall(r'\\b\\S*-\\W*\\S*\\b',text):\n",
    "    dashleft.append(i)\n",
    "set(dashleft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dashleft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these may need to be fixed, but it might be best to fix them in the next notebook, where we will replace the `-` with `_`, so that we can keep the compound words connected together. For example, `snap-sacks` might be a particular type of sack that should be distinguished from other sacks. If that's the case, then linking the `snap` to `sack` can be done with the underscore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are a few that you can clean up manually. We can use another function to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text = textreplacereplace('hyphen',changesdict,'Nuns-of','Nuns of',text)\n",
    "# changesdict['hyphen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when you use the `addextra` function, this is document-specific. But since the function starts with an \"if the error is in `text`...\", you don't need to worry about an error that occurs in document1 being added to document2's `changesdict`, unless the error actually exists in document2 as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_h = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syconpat'd replacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads another Underwood substitution lexicon as a dict, which substitutes the common early modern synocopated words with the full version: `doubl'd` into `doubled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (lexicapath + 'SyncopeRules_caps.csv','r') as subs:\n",
    "    reader = csv.reader(subs)\n",
    "    syncope = {rows[0]:rows[1] for rows in reader}\n",
    "syncope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(syncope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `syncopedict` with changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use lexicon function to make replacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['syncope'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('syncope',changesdict,syncope,text)\n",
    "changesdict['syncope']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many sycnopat'd words fixed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(changesdict['syncope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edits += 's'\n",
    "text_s = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyphenated words separated by line break(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With OCRed texts, words might be hyphenated at the end of a line. These need to be rejoined. Note that when you rejoin words, you might create words which then need some further correction. For example, rejoining `garr- ifon` will turn it into a long-s word, which in turn needs to be converted from `garrifon` to `garrison`. So you need to rejoin hyphenated words early on in the notebook, or else you'll need to rerun some of your code again. In general, think carefully (and experiment) to see the exact order in which the various corrections need to be executed. This is also why keeping track of the order in the `edits` variable is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r'\\-',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r'-\\n{1,4}[a-z]',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r'\\b\\S*-\\n{1,4}[a-z]\\S*\\b',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this correction is a bit more complicated, since you need to delete the hyphen and one or more line breaks. If it's only one return, that's straightforward enough. But if there are multiple returns, your code needs to be a bit more flexible. This code does that by using the `group` feature of regex, but it may need some work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T02:10:34.797630Z",
     "start_time": "2018-12-08T02:10:34.794938Z"
    }
   },
   "outputs": [],
   "source": [
    "changesdict['split'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T02:10:34.823561Z",
     "start_time": "2018-12-08T02:10:34.799529Z"
    }
   },
   "outputs": [],
   "source": [
    "whole = re.compile(r'\\b\\S*-\\n{1,4}[a-z]\\S*\\b')\n",
    "withpar = re.compile(r'(\\b\\S*)(-\\n{1,4})([a-z]\\S*\\b)')\n",
    "\n",
    "for i in re.findall(whole,text):\n",
    "    subs = re.sub(withpar,r'\\1\\3',i)\n",
    "    changesdict['split'][i] = subs\n",
    "changesdict['split']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'h3'\n",
    "text_h3 = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rejoin hyphenated words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find hyphenated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\S*\\- \\S*',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: Just deleting `- ` might be quickest, but there will be a couple of false positives, words that get compressed together which should be separate.\n",
    "\n",
    "Maybe use `if` second part starts with upper, just delete hyphen and not space? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in re.findall(r'(\\S*)(\\- )(\\S*)',text):\n",
    "    if i[2].istitle():\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyphennospace = {}\n",
    "wholehyphen = re.compile(r'\\S*\\- \\S*')\n",
    "parthyphen = re.compile(r'(\\S*)(\\- )(\\S*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in re.findall(wholehyphen,text):\n",
    "    for j in re.findall(r'(\\S*)(\\- )(\\S*)',i):\n",
    "        if j[2].istitle():\n",
    "            subs = re.sub(parthyphen,r'\\1-\\3',i)\n",
    "            hyphennospace[i] = subs\n",
    "        else:\n",
    "            subs = re.sub(parthyphen,r'\\1\\3',i)\n",
    "            hyphennospace[i] = subs\n",
    "hyphennospace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['hyphens'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('hyphens',changesdict,hyphennospace,text)\n",
    "changesdict['hyphens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(changesdict['hyphens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `CorrectionRules`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can again use the heavy lifting work provided by Underwood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to check the corrections to be made before you run the substitutions, you can do it with the code below. But this will probably take a few minutes, since it has to loop through 192,000 items across the entire text. Uncomment and run it if you're willing to wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for k,v in correct.items():\n",
    "#     if k in words:\n",
    "#         print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['correct1'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take a few minutes to run, depending on your computer, since it checks each of 192,000 items against every word in your `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('correct1',changesdict,correct,text)\n",
    "changesdict['correct1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many errors did the `CorrectionRules` lexicon catch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(changesdict['correct1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty effective - standing on the shoulders of giants, and all that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'c'\n",
    "text_c = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Long-s` substitutions (first pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we used code to replace the official long-s character (`ſ`) with an `f`. Now, we will replace all of the long-s `f`s with the `s`. We do this in several steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-s ambiguous pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems with converting long-s words is that English has some words that are ambiguous, i.e. without context, it's unclear whether the string `fail` should be the word `fail`, or rather the word `sail`. Ted Underwood's `AmbiguousPairs` lists about 400 possibilities, including various inflections (`fails`,`failed`,`failing`...).\n",
    "\n",
    "In order to automate this, we can proceed probabilistically, using frequences to make the best guess. To give an example, we can search through a large English corpus and count up the frequencies of `fail` and `sail` with ngrams, i.e. given the words surrounding this instance of `fail`, say, `to fail away`, is it more likely to be `fail` or `sail`? If there are examples of both ngrams, we can either choose the most likely choice, or we can display it to confirm by eye. I already created the frequencies for the long-f words, and saved it as a separate lexicon, which we can load here.\n",
    "\n",
    "For now, I've just removed the pairs that are ambiguous *given my corpus* from Underwood's long-s list. For example, in my military-political-diplomatic documents, `cafe` is extremely unlikely, whereas `case` was used *a lot*. Similarly, Jesus doesn't `fave`, he `save`(s), to `see` is much more likely than a `fee`, and so on. But another reason to maintain that audit trail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace long-s with lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can replace the remaining long-s words by using another Underwood lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (lexicapath + 'long_s_subs_caps.csv','r') as subs:\n",
    "    reader = csv.reader(subs)\n",
    "    longsedits = {rows[0]:rows[1] for rows in reader}\n",
    "longsedits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(longsedits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find long-s errors. The VEP versions have removed all the long-s words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for old,new in longsedits.items():\n",
    "    if old in text:\n",
    "        print(old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['longs'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('longs',changesdict,longsedits,text)\n",
    "changesdict['longs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in longsedits.keys():\n",
    "    if k.lower() in text:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any hits appear in the above code, you could make it into a list and then run your `lexiconreplaceassign` function on that list. Another way to deal with the problem of titlecase occurrences is to change the underlying lexicon file with Python: duplicate each item in the lexicon and titlecase the duplicate. Alternately, you could use the `title` method in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edits += 'f'\n",
    "text_f = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete periods from dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to fix if we want to do sentence tokenization, e.g. `On the 18th. of May...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dateperiod = []\n",
    "for i in re.findall(r'\\S*\\b \\d{1,2}th\\.\\s*\\S*\\b',text):\n",
    "    dateperiod.append(i)\n",
    "dateperiod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dateperiod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace and check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dateperiod:\n",
    "    text = re.sub(r'(\\d{1,2})th.','\\g<1>th ',text)\n",
    "re.findall(r'\\S*\\b \\d{1,2}th\\.\\s*\\S*\\b',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_p = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace double-spaces with single space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r'  ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('  ',' ')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text__ = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove extra line breaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless multiple line breaks have a special meaning in your document, you can collapse them down to a single return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linebreak = re.findall(r'\\n{2,5}', text)\n",
    "linebreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(linebreak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably don't need to add these before and after changes to `changedict`, so we can just use a simple loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in re.findall(r'\\n{2,5}',text):\n",
    "    text = re.sub(i,r'\\n',text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, however, add the change to `edits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'r'\n",
    "text_r = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove all `\\n` separating sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have one or more line breaks between parts of a sentence. Our previous replacement of multiple returns with a single return should have left us with only one line break in a row. Now we can diagnose where they occur, and decide if they need to be removed in order to rejoin a sentence split across a line break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T02:10:34.968756Z",
     "start_time": "2018-12-08T02:10:20.210Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "premature = re.findall(r'\\b\\w*? {0,1}\\n {0,1}[a-z]+?\\b',text)\n",
    "for i in enumerate(premature):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually change premature line breaks as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T02:10:34.972890Z",
     "start_time": "2018-12-08T02:10:20.225Z"
    }
   },
   "outputs": [],
   "source": [
    "#text = addextra()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pre = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate save text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go further, we might as well save an intermediate copy of the revised text. Just be sure to name it very clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processedpath + filename + '_' + edits + '_intermediate.txt', 'w', encoding = 'UTF-8') as export:\n",
    "    export.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with word dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a cleaner version of the document, we should do a broader search for remaining errors. We can tokenize our text and compare the tokens with an English language dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new list of the tokens in the corrected text, excluding the specified characters (i.e. punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsdirty = word_tokenize(text)\n",
    "wordsdirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wordsdirty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminate all punctuation from `wordsdirty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = [i for i in wordsdirty if i not in (',','-',';',':','.','’','&','#','$','!','%','\\'','*','•','(',')')]\n",
    "tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the `set` of the above list to see how many \"types\" (i.e. unique tokens) are in the revised token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2set = set(tokens2)\n",
    "tokens2set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens2set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, 4,000 unique words (\"types\") occurring a total of 37,000 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in eMOP English dictionary to check spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to check our corrected text against a list of English words from the period. We'll use the Early Modern OCR Project (EMOP) word list: https://github.com/Early-Modern-OCR/TesseractTraining. It's a relatively small word dictionary of about 121,000 words. Using a large dictionary (you can find English word lists of 300,000+) will make it more difficult to make accurate corrections, because there are lots of obscure (and modern) words that our c.1700 sources were highly unlikely to use. So we'll keep the 'master dictionary' small, and add more words from our specific domain (in my case, military terminology, early modern French and English titles and ranks...) as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read word list in. It will be used to find 'real' words that don't need to be corrected further, and flag words that we should check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lexicapath + 'eMOP_en_dict_edited.txt', 'r',encoding='UTF-8') as f:\n",
    "    emop_en = f.read().split('\\n')\n",
    "emop_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(emop_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find non-`emop_en` tokens in `tokens2set`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our `emop_en`, we can find tokens that are *not* in the list and decide how we want to deal with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notinemop = []\n",
    "for i in tokens2:\n",
    "    if i.lower() not in emop_en: # note lowercasing each token to match emop format\n",
    "        notinemop.append(i)\n",
    "notinemop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(notinemop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(notinemop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be many unique tokens not in `emop_en`, but this is as much an issue with the emop dictionary as with the errors in our source. Some of these 'non-words' might be digits and punctuation (brackets especially). Others might be valid proper nouns - personal names, places, organzations, etc. Some might actually be parts of a compound term that will get fixed once `underscore` substitutions are made in the next notebook.\n",
    "\n",
    "We can get a better sense by looking at the unique tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notinemop1 = sorted(set(notinemop))\n",
    "notinemop1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're only looking for questionable words at this point, let's eliminate digits and punctuation from the list, to focus on the words. Note that the titlecase words are alphabetically sorted *before* their lowercase brethren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notinemopwords = []\n",
    "for i in notinemop1:\n",
    "    if i.isalpha():\n",
    "        notinemopwords.append(i)\n",
    "notinemopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(notinemopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write questionable words to txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a copy of the above to a separate txt file, in case we want to look at it elsewhere. We might, for example, want to add these items to another lexicon, or to the dictionary, and then read it in and run the corrections using that `Set flag` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tocheckpath + filename + '_words_to_check.txt','w',encoding = 'UTF-8') as f:\n",
    "    for i in notinemopwords:\n",
    "        f.write(\"%s\\n\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to find the context of a word from the above list. For example, we might be confused that `fortifie` is still in `text`, even though `fortifie` is in the `CorrectionRules` lexicon. So we can explore its context to figure out why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r'\\b\\S*\\W*fortifie\\W*\\S*\\b',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now realize that I'd padded `fortifie` on both sides with spaces in the lexicon, in order to not inadvertently replace substrings, like turning `fortified` into `fortifyd`. Yet that padding made the code miss `fortifie,` with a trailing comma instead of a space.\n",
    "\n",
    "If you want to make changes on text, rather than just tokens, issues of substrings and inflections and capitalization will loom large in your decisions: `fortify`, `fortifies`, `fortified`, `fortifying`, `fortification`, `fortifications`, `Fortify`, `fortification.`...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you wanted to get a count of how many times a particular (sub)string appears in another string, instead of creating a list and checking its `len`, you can use the `count` method in the `text` string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.count('fortifie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct questionable words manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can correct any of the above that seem peculiar, with our `textreplace` function; errors that are likely to reappear in other documents should be added to the appropriate lexicon.\n",
    "\n",
    "But let's hold off on that for a minute, until we deal with the proverbial elephant in the room."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with proper nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skimming through the items listed above, we can see that many of the items are not errors, but rather proper nouns, which are, not surprisingly, absent from a generic English word dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll deal with the messy proper nouns later, i.e. standardize their spelling, so `Anverquerque` is the same as `Auverquerque` is the same as `Overkirk` is the same as `Ouwerkerk`. But for now, let's eliminate the proper nouns from our list to focus on words that are most-likely mispelled and need correction. This is easiest if you already have a list of proper nouns that are likely to appear in your document. Until your subfield creates its own lexica to share, it may take you a while to create your own. You can start by cobbling together lists of people, places and organizations from your own resources. You can also use Python code to find titlecased words in your corpus as candidates, and you can also find lists, like gazetteers, online. Perhaps even a scanned book index or two might help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in proper names lexicon, found online and added to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lexicapath + 'ProperNames.csv', 'r',encoding='UTF-8') as f:\n",
    "    proper = f.read().split('\\n')\n",
    "proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(proper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now combine our dictionary and proper nouns lists together, and lowercase them. We only do this in code (i.e. not combine the two lexica together in a file) because we might want to consider them separately. It's also a matter of version control: if, for example, we want to add a new person, we'll need to remember to add it to both the dictionary file and to the proper noun file. Over time, that may lead to the various lexica getting out of sync, with one version including recent additions while the other doesn't. Better to keep them as separate lists, and combine them together in your code when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combowords = []\n",
    "for i in emop_en:\n",
    "    combowords.append(i.lower())\n",
    "for i in proper:\n",
    "    combowords.append(i.lower())\n",
    "combowords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we lowercased the above words, because we'll want to lowercase our questionable words. This will allow us to find matches regardless of the capitalization of any specific token, whether the token is at the beginning of a sentence, in the middle, or whether the author used old-fashioned Weird Capitalization in the middle of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combowords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can see which tokens in our document are not in this combined list (words only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notindict = []\n",
    "for i in notinemopwords:\n",
    "    if i.lower() not in combowords: #notice the lowercasing for comparative purposes only\n",
    "        notindict.append(i)\n",
    "notindict = sorted(notindict)\n",
    "notindict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(notindict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the size of the list, we can check it and look for things to fix:\n",
    "1. Spelling errors to fix\n",
    "2. Names to add to `ProperNouns`. We may want to hold off on these for the moment, till we figure out a better way to correct them programmatically.\n",
    "2. Technical terms to add to `emop_en`, or a specialized lexicon if you prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to see how many are likely proper nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in notindict:\n",
    "    if i.istitle():\n",
    "        count += 1\n",
    "print(count)\n",
    "print(count/len(notindict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['notindict'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we still have a lot of place names, which means they need to be added to your `propernames` list. \n",
    "\n",
    "But first, consider how you want to standardize them, and how that will be implemented. For me, I want all spelling variations standardized to the modern name, to make it easier to automatically look up the coordinates (geocode). But I'll also want to underscore compond proper nouns in order to keep their tokens together, e.g. `Saint-Omer` and `Saint Omer` will both become `Saint_Omer`. Those tasks will be taken care of in another notebook, so for now I'll simply add any spelling variations I see in the `notindict` list to the `CorrectionRules` lexicon, and then rerun the whole notebook to update the changes. Other lexica will be dedicated to replacing those standardized names with the underscored version. If your sources are like mine, you'll also need to decide what to do about exonyms, i.e. foreign names for toponyms, like `Boiled Duck` for `Bois-le-Duc` for `s'Hertogenbosch` or `Den Bosch`. These exonyms themselves can vary according to contemporaries' irregular spelling practices, which scholars are trying to solve with fuzzy matching and machine learning. Standardizing all these to a single modern spelling will require making a (semi-)arbitrary decision about which is the \"offical\" version. Linked Open Data resources increasingly offer you the option of translating from one to another, if you know how to incorporate that into your workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to find the context of a specific word, you can use some simple regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\b\\S*\\W*Chemin\\W*\\S*\\b',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need more context, you can add additional `\\S* ` on either side. For more power, though, you can convert the `text` into an NLTK `Text` object and get the concordance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltkwords = word_tokenize(text)\n",
    "nltkwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltktext = Text(nltkwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltktext.concordance(\"Caesar\",lines=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write `notindict` to `csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a copy of the above revised list to another txt file, in case we want to look at it elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tocheckpath + filename + '_addtoproper.csv','w') as f:\n",
    "    for item in notindict:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doubly doubly entered words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we'll find words that appear twice in a row, e.g. `of of`. It's easy for our brains to skip past them, especially if they're stopwords, so let's let the computer find them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll use the `enumerate` method on the `tokens` list. We don't want to use the `words` token list, because that has removed certain characters, so any doubled-up words might actually be separated by punctuation. So let's retokenize the revised text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_revised = nltk.word_tokenize(text)\n",
    "token_revised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to check for duplicated words is to split the tokens up into bigrams, each word paired with its following neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.bigrams(token_revised)\n",
    "bigram = []\n",
    "for bi in bigrams:\n",
    "    bigram.append(bi)\n",
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a list of tuples. Now we can loop through each tuple and see if the first and second parts are the same. If they are, save them to another list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubly = []\n",
    "for i in bigram:\n",
    "    if i[0] == i[1]:\n",
    "        doubly.append(i)\n",
    "doubly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doubly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some of these are proper nouns, which we'd think might not be as likely to be accidentally duplicated, we should check to see if all of the above are actually in the text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: Not all of the below list items are actually neighbors, given tokenizing issues. `changesdict` will only replace those that are real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doublystr = []\n",
    "for i in doubly:\n",
    "    dubstr = ' '.join(i) #join method converts the bigram tuple elements into str, so we can use regex\n",
    "    #doublystr.append(re.findall(r'\\S*' + dubstr,i))\n",
    "    #test = re.findall(r'\\S*' + dubstr,i)\n",
    "    if re.search(r'[a-zA-Z]',dubstr):\n",
    "        doublystr.append(dubstr)\n",
    "doublystr = sorted(set(doublystr))    \n",
    "doublystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doublystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doublydict = {}\n",
    "for i in doublystr:\n",
    "    doublydict[i] = ' ' + i.split()[0] + ' ' #split list item and only keep first element\n",
    "doublydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['doubly'] = {}\n",
    "text = lexiconreplaceassign('doubly',changesdict,doublydict,text)\n",
    "changesdict['doubly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += '2'\n",
    "text_2 = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for other `possiblenonwords`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in stopword list and lexica, so only find unexpected words.\n",
    "\n",
    "We'll write these to a separate file and go through them at our leisure, and then make the substitutions when `newdoc` == `N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emop_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(emop_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lexicapath + 'stopwords_en_edited.txt', 'r',encoding='UTF-8') as f:\n",
    "    stops = f.read().split('\\n')\n",
    "stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = list(sorted(set(emop_en + stops + proper)))\n",
    "allwords1 = []\n",
    "for i in allwords:\n",
    "    if i.isalpha():\n",
    "        allwords1.append(i.lower())\n",
    "allwords1 = sorted(allwords1)\n",
    "allwords1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allwords1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenset1 = []\n",
    "tokenset = list(sorted(set(token_revised)))\n",
    "for i in tokenset:\n",
    "    if i.isalpha():\n",
    "        tokenset1.append(i.lower()) # lower gets rid of cased- duplicates\n",
    "tokenset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(tokenset1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python list magic to find those in list1 not in list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_list = list(set(tokenset1) - set(allwords1))\n",
    "main_list = sorted(main_list)\n",
    "main_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(main_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_list_title = [i.title() for i in main_list]\n",
    "main_list_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(main_list_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possiblenonwords = list(set(main_list)-set(proper))\n",
    "possiblenonwords = sorted(possiblenonwords)\n",
    "possiblenonwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(possiblenonwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we'd make the changes to `possiblenonwords1` and rerun this notebook with the flag set to `newdoc == 'N'`. But this list might have hundreds of possibilities to go through, many of which might be proper nouns that should be added to another lexicon. So for now, we'll just leave it commented out for now, and, after you've gone through the list, you can uncomment this cell and rerun the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tocheckpath + filename + '_possiblenonwords.txt', 'w', encoding = 'UTF-8') as f:\n",
    "    for item in possiblenonwords:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if newdoc == 'N':\n",
    "#     with open (tocheckpath + filename + '_possiblenonwords1.txt','r') as subs:\n",
    "#             possnonword = subs.read().split('\\n')\n",
    "#     changesdict['possnonword'] ={}\n",
    "#     text = lexiconreplaceassign('possnonword',changesdict,possnonword,text)\n",
    "#     changesdict['possnonword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if newdoc == 'N':\n",
    "    edits += 'p'\n",
    "    text_p = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write `_page` version of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, if you are analyzing an entire text, or if your analysis doesn't depend on pagination, you could make your text one long string. Otherwise, for example, page numbers might be included when you are searching for numbers used by the author in the text. You can always make another copy of the text with the pagination, as backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputpath + filename + '_late_pages.txt', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete line breaks `\\n`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to delete all of the extra line breaks, you can use a `list comprehension` to save each line as an item in a list. You then `join` them back together at the very end, back into a string, with a space (or whatever delimiter you want) in between each item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: \n",
    "1. This will eliminate any page information. If you want to analyze anything by page, skip this step.\n",
    "2. If you also want to get rid of paragraph breaks, you can `split` on only a single line break, rather than two.\n",
    "3. If your text has words that were separated (i.e. hyphenated) across a page break, this will get them closer together, but you'll need to rerun the hyphenation correction. And you might even need to rerun the other corrections if, for example, that hyphenated word split across two pages was spelled wrong, e.g.\n",
    "\n",
    "`Anver-`\n",
    "\n",
    "`querque` should really be `Anverquerque`, which should really be `Auverquerque`, which, frankly, should really be `Overkirk` - but that's for another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lines = [line for line in text.split('\\n\\n') if line.strip()]\n",
    "text_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: I named this version `text2`, in case you want to retain both it and `text` in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = ' '.join(text_lines)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits = edits + 'l'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete page numbers `[\\d]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is also optional, depending on your purposes, and whether you are focused only on the content written by the author, vs. being interested in the layout of the content on the printed page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: If you want to analyze your text by sentence, consider getting rid of the page numbers, since many sentences are split up by line breaks and a page number. This might also be a concern if there is a word that is hyphenated at the bottom of a page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagedict = {}\n",
    "page = re.findall(r'\\n\\[\\d{1,4}\\]\\n',text)\n",
    "for i in page:\n",
    "    pagedict[i] = ''\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'\\n\\[\\d{1,4}\\]\\n',' ',text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete `[unnumbered]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnumdict = {}\n",
    "page = re.findall(r'\\[unnumbered\\]\\n',text)\n",
    "for i in page:\n",
    "    unnumdict[i] = ''\n",
    "    print(page.index(i),i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'\\[unnumbered\\]\\n','',text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete `[page]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagedict = {}\n",
    "page = re.findall(r'\\[page\\]',text)\n",
    "for i in page:\n",
    "    pagedict[i] = ''\n",
    "    print(page.index(i),i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'\\[page\\]','',text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete `Page #`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = re.findall(r'\\n[pP]age \\d{1,4}\\n{1,2}',text)\n",
    "for i in page:\n",
    "    pagedict[i] = ''\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'\\n[pP]age \\d{1,4}\\n{1,2}',' ',text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'p'\n",
    "text_pg = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete extra line breaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes line breaks split up sentences. Also an issue if word needing correcting is at beginning of line and it requires a padded space in lexicon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r'\\S*[^\\s\\.\\?]\\n\\S*',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitline = {}\n",
    "for i in re.findall(r'\\S*[^\\s\\.\\?]\\n\\S*',text):\n",
    "    splitline[i] = i.replace('\\n',' ')\n",
    "splitline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "changesdict['splitline'] = {}\n",
    "text = lexiconreplaceassign('splitline',changesdict,splitline,text)\n",
    "changesdict['splitline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'n'\n",
    "text_sl = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Correction_Rules` again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be a better way to do this, but a few extra seconds isn't that big a deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdict['correct2'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lexiconreplaceassign('correct2',changesdict,correct,text)\n",
    "changesdict['correct2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits += 'c'\n",
    "text_c2 = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write `no_pages` output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to preserve a version of the corrected text without the page breaks, you can do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputpath + filename + '_late_nopages.txt', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for any duplicate letters added by `Correction`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue: `xamin'd` -> `examined` without padding leads to `eexamined`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: Some acceptable:\n",
    "1. `i`: Roman numerals; Latin plural (e.g. `Imperii`)\n",
    "2. Any letters in the middle of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r'\\b\\S*([a-z])\\1{2,}\\S*',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\be{2}\\S*',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `changedict` stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to the interesting part. here we can how many corrections were made for each type of error. Remember that since this code has been run on multiple documents with different types of errors, each document will likely only have a subset of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate how many changes in each revision subdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesdictstats = {}\n",
    "for k,v in changesdict.items():\n",
    "    changesdictstats[k] = len(v)\n",
    "changesdictstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're practically done! At least with these first steps. In case you want to look through your cleaner text one last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start saving the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleaned `text` to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T02:10:35.033876Z",
     "start_time": "2018-12-08T02:10:20.466Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(outputpath + filename + '_clean_nb1_' + edits + '_nb1.txt', 'w', encoding = 'UTF-8') as export:\n",
    "    export.write(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save `changedict` as audit trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T02:10:35.034485Z",
     "start_time": "2018-12-08T02:10:20.470Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(outputpath + filename + '_changesdict_' + edits + '_nb1.csv', 'w', encoding = 'UTF-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for k,v in changesdict.items():\n",
    "            writer.writerow([k, v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To come: notebook 2, for more transformative changes. Things like standardizing proper nouns, eliminating weird archaic capitalization, and so much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some additional code if you want, for example, to standardize all those unsightly `-eth` verb endings to the appropriate 3rd person singular form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find remaining `-eth` words in cleaned text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retokenize last version `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words1 = word_tokenize(text)\n",
    "words1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find any tokens ending with `-eth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethtoken = []\n",
    "for token in words1:\n",
    "    if token.endswith('eth'):\n",
    "        ethtoken.append(token)\n",
    "ethtoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethtoke = sorted(set(ethtoken))\n",
    "ethtoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ethtoke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all `-eth` words to add to `Corrections`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add these `-eth` words to a lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lexicapath + \"albemarle1671_addtoproper.csv\", 'r',encoding='UTF-8') as f:\n",
    "    eth = f.read().split('\\n')\n",
    "eth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethtos = []\n",
    "for i in eth:\n",
    "    if i.endswith('eth'):\n",
    "        ethtos.append(i)\n",
    "ethtos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ethtos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn into a dict to save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethdict = {}\n",
    "for i in ethtoke:\n",
    "    ethdict[i] = i.replace('eth','es')\n",
    "ethdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add titlecase versions to `ethdict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (textpath + 'ethtofix.csv','r') as subs:\n",
    "    reader = csv.reader(subs)\n",
    "    ethtofix = {rows[0]:rows[1] for rows in reader} # 1st row as key, 2nd row as value\n",
    "ethtofix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethdict1 = {}\n",
    "for k,v in ethtofix.items():\n",
    "    ethdict1[k] = v\n",
    "    ethdict1[k.title()] = v.title()\n",
    "ethdict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ethdict1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to csv and then clean further - which more common, `-es` or `-s`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputpath + 'eth_fixed1.csv', 'w', encoding = 'UTF-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for k,v in ethdict.items():\n",
    "            writer.writerow([k, v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text editor, add extra comma before line break so will match csv of `Correction_Rules`"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "232px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 458.46666600000003,
   "position": {
    "height": "480px",
    "left": "507.4px",
    "right": "157.533px",
    "top": "112px",
    "width": "549px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
